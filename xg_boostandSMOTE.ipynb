{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install pandas\n",
    "!python -m pip install scikit-learn\n",
    "!python -m pip install imblearn\n",
    "!python -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\Juliana\\OneDrive\\Documents\\GitHub\\Group3_AI4ALL-Project\\Data\\Miami_Accidents(Clean).csv\")\n",
    "df = df.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juliana\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [20:10:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.977088948787062\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.45      0.71      0.56         7\n",
      "           2       0.99      0.99      0.99     23641\n",
      "           3       0.50      0.61      0.55       512\n",
      "           4       0.94      0.90      0.92       326\n",
      "\n",
      "    accuracy                           0.98     24486\n",
      "   macro avg       0.72      0.80      0.75     24486\n",
      "weighted avg       0.98      0.98      0.98     24486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Assuming 'df' is your DataFrame which already contains your data.\n",
    "\n",
    "# Define numerical, categorical, and text columns for preprocessing\n",
    "numerical_cols = ['Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', \n",
    "                  'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', \n",
    "                  'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n",
    "\n",
    "categorical_cols = ['Street', 'Zipcode', 'Weather_Condition', 'Sunrise_Sunset']\n",
    "\n",
    "text_col = 'Description'  # Text column for feature extraction\n",
    "\n",
    "# Set up the ColumnTransformer for preprocessing\n",
    "# This will apply different transformations to different types of data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),  # Normalize numerical data\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),  # Convert categorical data to one-hot encoded vectors\n",
    "        ('txt', CountVectorizer(), text_col)  # Convert text data into a matrix of token counts\n",
    "    ],\n",
    "    remainder='drop'  # Ignore columns that are not specified\n",
    ")\n",
    "\n",
    "# Create the pipeline with preprocessing, SMOTE, and XGBClassifier\n",
    "model_pipeline = ImbPipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # First apply the preprocessor defined above\n",
    "    ('smote', SMOTE(random_state=42)),  # Then apply SMOTE to handle class imbalance\n",
    "    ('classifier', XGBClassifier(  # Finally, use XGBClassifier to train the model\n",
    "        n_estimators=100,\n",
    "        scale_pos_weight=3,  # Adjust this based on the ratio of majority to minority class instances\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Prepare features and target variable\n",
    "X = df.drop('Severity', axis=1)  # Features\n",
    "y = df['Severity'] - 1  # Target variable, adjust classes to start from 0\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model using the pipeline\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions (remember to adjust back the predictions to original class labels)\n",
    "predictions_adjusted = model_pipeline.predict(X_test)\n",
    "predictions = predictions_adjusted + 1  # Adjust predictions to match original class labels\n",
    "\n",
    "# Evaluate the model using the original class labels\n",
    "print(\"Accuracy:\", accuracy_score(y_test + 1, predictions))  # Adjust y_test to match original labels for comparison\n",
    "print(\"Classification Report:\\n\", classification_report(y_test + 1, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.71      0.56         7\n",
      "           1       0.99      0.99      0.99     23641\n",
      "           2       0.50      0.61      0.55       512\n",
      "           3       0.94      0.90      0.92       326\n",
      "\n",
      "    accuracy                           0.98     24486\n",
      "   macro avg       0.72      0.80      0.75     24486\n",
      "weighted avg       0.98      0.98      0.98     24486\n",
      "\n",
      "Matriz de Confusi√≥n:\n",
      " [[    5     1     1     0]\n",
      " [    6 23314   303    18]\n",
      " [    0   198   313     1]\n",
      " [    0    30     3   293]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred =model_pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[1225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should be 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted severity is: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Datos para predecir\n",
    "new_data_to_predict = {\n",
    "    'Start_Time': ['2016-10-17 23:23:59'],\n",
    "    'End_Time': ['2016-10-17 23:23:59'],\n",
    "    'Start_Lat': [25.785738],\n",
    "    'Start_Lng': [-80.175632],\n",
    "    'End_Lat': [25.78575],\n",
    "    'End_Lng': [-80.17564],\n",
    "    'Distance(mi)': [0.001],\n",
    "    'Description': ['Closed at Trooper Robert G Smith Brg - Road closed due to accident.'],\n",
    "    'Street': ['MacArthur Cswy W'],\n",
    "    'Zipcode': ['33132'],\n",
    "    'Temperature(F)': [80.1],\n",
    "    'Wind_Chill(F)': [78.486101],\n",
    "    'Humidity(%)': [64.0],\n",
    "    'Pressure(in)': [29.93],\n",
    "    'Visibility(mi)': [10.0],\n",
    "    'Wind_Speed(mph)': [6.9],\n",
    "    'Precipitation(in)': [0.009293],\n",
    "    'Weather_Condition': ['Partly Cloudy'],\n",
    "    'Bump': [False],\n",
    "    'Crossing': [True],\n",
    "    'Give_Way': [False],\n",
    "    'Junction': [False],\n",
    "    'No_Exit': [True],\n",
    "    'Railway': [False],\n",
    "    'Roundabout': [False],\n",
    "    'Station': [False],\n",
    "    'Stop': [False],\n",
    "    'Traffic_Calming': [False],\n",
    "    'Traffic_Signal': [False],\n",
    "    'Turning_Loop': [False],\n",
    "    'Sunrise_Sunset': ['Night']\n",
    "}\n",
    "\n",
    "\n",
    "new_data_df = pd.DataFrame(new_data_to_predict)\n",
    "\n",
    "predicted_severity = model_pipeline.predict(new_data_df)\n",
    "\n",
    "\n",
    "predicted_severity = predicted_severity + 1\n",
    "\n",
    "# \n",
    "print('The predicted severity is:', predicted_severity[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "should be 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted severity is: 3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_data_to_predict = {\n",
    "    'Start_Time': ['11/30/2016 16:40:31'],\n",
    "    'End_Time': ['11/30/2016 17:10:19'],\n",
    "    'Start_Lat': [25.78601],\n",
    "    'Start_Lng': [-80.25809],\n",
    "    'End_Lat': [25.78336],\n",
    "    'End_Lng': [-80.26911],\n",
    "    'Distance(mi)': [0.71],\n",
    "    'Description': ['At SR-953/42nd Ave/Le Jeune Rd - Accident.'],\n",
    "    'Street': ['Dolphin Expy W'],\n",
    "    'Zipcode': ['33126'],\n",
    "    'Temperature(F)': [78.1],\n",
    "    'Wind_Chill(F)': [78.486101],\n",
    "    'Humidity(%)': [76],\n",
    "    'Pressure(in)': [29.96],\n",
    "    'Visibility(mi)': [10],\n",
    "    'Wind_Speed(mph)': [11.5],\n",
    "    'Precipitation(in)': [0.009293],\n",
    "    'Weather_Condition': ['Mostly Cloudy'],\n",
    "    'Bump': [False],\n",
    "    'Crossing': [False],\n",
    "    'Give_Way': [False],\n",
    "    'Junction': [False],\n",
    "    'No_Exit': [False],\n",
    "    'Railway': [False],\n",
    "    'Roundabout': [False],\n",
    "    'Station': [False],\n",
    "    'Stop': [False],\n",
    "    'Traffic_Calming': [False],\n",
    "    'Traffic_Signal': [False],\n",
    "    'Turning_Loop': [False],\n",
    "    'Sunrise_Sunset': ['Day']\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a DataFrame\n",
    "new_data_df = pd.DataFrame(new_data_to_predict)\n",
    "\n",
    "# Using the pipeline to make the forecast\n",
    "predicted_severity = model_pipeline.predict(new_data_df)\n",
    "\n",
    "# Adjust the prediction to the original class scale, if needed\n",
    "predicted_severity = predicted_severity + 1  \n",
    "\n",
    "print('The predicted severity is:', predicted_severity[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
